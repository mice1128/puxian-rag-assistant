#!/usr/bin/env python3
"""
æµ‹è¯• Qwen æ¨¡å‹å¯¹è¯
ç›´æ¥å‘½ä»¤è¡Œäº¤äº’æµ‹è¯•
"""
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../backend'))

from app.services.qwen_service import get_qwen_service
import torch

def main():
    print("=" * 60)
    print("ğŸ¤– Qwen æ¨¡å‹å¯¹è¯æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥ CUDA
    print(f"\nè®¾å¤‡ä¿¡æ¯:")
    print(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  GPU æ•°é‡: {torch.cuda.device_count()}")
        print(f"  å½“å‰ GPU: {torch.cuda.current_device()}")
        print(f"  GPU åç§°: {torch.cuda.get_device_name(0)}")
    
    # åˆå§‹åŒ–æœåŠ¡
    print("\nåˆå§‹åŒ– Qwen æœåŠ¡...")
    qwen = get_qwen_service()
    print(f"  æ¨¡å‹è·¯å¾„: {qwen.model_path}")
    print(f"  è®¾å¤‡: {qwen.device}")
    
    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼Œè¯·ç¨å€™ï¼‰...")
    qwen.load_model()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    
    print("=" * 60)
    print("å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºï¼‰")
    print("=" * 60)
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            question = input("\nä½ : ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['exit', 'quit', 'q']:
                print("\nå†è§ï¼")
                break
            
            # ç”Ÿæˆå›ç­”
            print("\nåŠ©æ‰‹: ", end="", flush=True)
            
            response, tokens = qwen.generate(
                question,
                max_new_tokens=512,
                temperature=0.7
            )
            
            print(response)
            print(f"\n[Token æ•°: {tokens}]")
            
        except KeyboardInterrupt:
            print("\n\nå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
