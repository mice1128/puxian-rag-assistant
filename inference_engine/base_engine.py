#!/usr/bin/env python3
"""
æŽ¨ç†å¼•æ“ŽæŠ½è±¡åŸºç±»
"""
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
import time


class BaseInferenceEngine(ABC):
    """æŽ¨ç†å¼•æ“ŽæŠ½è±¡åŸºç±»"""
    
    def __init__(self, config):
        """åˆå§‹åŒ–æŽ¨ç†å¼•æ“Ž"""
        self.config = config
        self.model = None
        self.is_loaded = False
        
    @abstractmethod
    def load_model(self):
        """åŠ è½½æ¨¡åž‹"""
        pass
    
    @abstractmethod
    def generate(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus sampling å‚æ•°
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
            
        Returns:
            {
                'text': ç”Ÿæˆçš„æ–‡æœ¬,
                'tokens': token æ•°é‡,
                'latency': å»¶è¿Ÿ(ç§’),
                'throughput': åžåé‡(tokens/s)
            }
        """
        pass
    
    @abstractmethod
    def batch_generate(
        self,
        prompts: List[str],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ç”Ÿæˆ
        
        Args:
            prompts: è¾“å…¥æç¤ºè¯åˆ—è¡¨
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æžœåˆ—è¡¨
        """
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """èŽ·å–æ¨¡åž‹ä¿¡æ¯"""
        return {
            'model_path': self.config.model_path,
            'gpu_id': self.config.gpu_id,
            'is_loaded': self.is_loaded,
            'backend': self.__class__.__name__,
        }
    
    def warmup(self, num_iterations: int = 3):
        """é¢„çƒ­æ¨¡åž‹"""
        print(f"ðŸ”¥ é¢„çƒ­æ¨¡åž‹ ({num_iterations} æ¬¡)...")
        warmup_prompt = "ä½ å¥½"
        
        for i in range(num_iterations):
            self.generate(warmup_prompt, max_tokens=10)
            print(f"  é¢„çƒ­ {i+1}/{num_iterations} å®Œæˆ")
        
        print("âœ… é¢„çƒ­å®Œæˆ")
    
    def benchmark(
        self,
        prompts: List[str],
        num_runs: int = 3,
        warmup: bool = True
    ) -> Dict[str, Any]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            prompts: æµ‹è¯•æç¤ºè¯åˆ—è¡¨
            num_runs: æ¯ä¸ªæç¤ºè¯è¿è¡Œæ¬¡æ•°
            warmup: æ˜¯å¦é¢„çƒ­
            
        Returns:
            æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
        """
        if warmup and num_runs > 0:
            self.warmup()
        
        results = []
        
        for prompt in prompts:
            prompt_results = []
            
            for run in range(num_runs):
                result = self.generate(prompt)
                prompt_results.append(result)
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_latency = sum(r['latency'] for r in prompt_results) / len(prompt_results)
            avg_throughput = sum(r['throughput'] for r in prompt_results) / len(prompt_results)
            
            results.append({
                'prompt': prompt[:50] + '...' if len(prompt) > 50 else prompt,
                'avg_latency': round(avg_latency, 3),
                'avg_throughput': round(avg_throughput, 1),
                'runs': num_runs
            })
        
        # æ€»ä½“ç»Ÿè®¡
        overall_latency = sum(r['avg_latency'] for r in results) / len(results)
        overall_throughput = sum(r['avg_throughput'] for r in results) / len(results)
        
        return {
            'backend': self.__class__.__name__,
            'num_prompts': len(prompts),
            'num_runs': num_runs,
            'overall_avg_latency': round(overall_latency, 3),
            'overall_avg_throughput': round(overall_throughput, 1),
            'details': results
        }
