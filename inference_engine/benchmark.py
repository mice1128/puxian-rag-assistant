#!/usr/bin/env python3
"""
æ¨ç†å¼•æ“æ€§èƒ½å¯¹æ¯”æµ‹è¯•
"""
import sys
import os
import json
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from inference_engine import TransformersEngine, VLLMEngine
from inference_engine.config import InferenceConfig


def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(title)
    print("=" * 80)


def compare_engines(config: InferenceConfig, test_prompts: list, num_runs: int = 3):
    """å¯¹æ¯”ä¸¤ç§æ¨ç†å¼•æ“"""
    print_section("ğŸ”¬ æ¨ç†å¼•æ“æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    
    print(f"\né…ç½®:")
    print(f"  æ¨¡å‹: {config.model_path}")
    print(f"  GPU: {config.gpu_id}")
    print(f"  æµ‹è¯•æç¤ºè¯æ•°: {len(test_prompts)}")
    print(f"  æ¯ä¸ªæç¤ºè¯è¿è¡Œæ¬¡æ•°: {num_runs}")
    
    results = {}
    
    # æµ‹è¯• Transformers å¼•æ“
    print_section("1ï¸âƒ£  Transformers åŸç”Ÿå¼•æ“")
    try:
        tf_engine = TransformersEngine(config)
        tf_engine.load_model()
        
        tf_results = tf_engine.benchmark(test_prompts, num_runs=num_runs, warmup=True)
        results['transformers'] = tf_results
        
        print(f"\næ€»ä½“æ€§èƒ½:")
        print(f"  å¹³å‡å»¶è¿Ÿ: {tf_results['overall_avg_latency']:.3f}s")
        print(f"  å¹³å‡åå: {tf_results['overall_avg_throughput']:.1f} tokens/s")
        
    except Exception as e:
        print(f"âŒ Transformers å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        results['transformers'] = {'error': str(e)}
    
    # æµ‹è¯• vLLM å¼•æ“
    print_section("2ï¸âƒ£  vLLM ä¼˜åŒ–å¼•æ“")
    try:
        vllm_engine = VLLMEngine(config)
        vllm_engine.load_model()
        
        vllm_results = vllm_engine.benchmark(test_prompts, num_runs=num_runs, warmup=True)
        results['vllm'] = vllm_results
        
        print(f"\næ€»ä½“æ€§èƒ½:")
        print(f"  å¹³å‡å»¶è¿Ÿ: {vllm_results['overall_avg_latency']:.3f}s")
        print(f"  å¹³å‡åå: {vllm_results['overall_avg_throughput']:.1f} tokens/s")
        
    except Exception as e:
        print(f"âŒ vLLM å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        print(f"   æç¤º: è¯·å…ˆå®‰è£… vLLM: pip install vllm")
        results['vllm'] = {'error': str(e)}
    
    # æ€§èƒ½å¯¹æ¯”
    if 'error' not in results.get('transformers', {}) and 'error' not in results.get('vllm', {}):
        print_section("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        
        tf_latency = results['transformers']['overall_avg_latency']
        vllm_latency = results['vllm']['overall_avg_latency']
        
        tf_throughput = results['transformers']['overall_avg_throughput']
        vllm_throughput = results['vllm']['overall_avg_throughput']
        
        latency_speedup = tf_latency / vllm_latency if vllm_latency > 0 else 0
        throughput_speedup = vllm_throughput / tf_throughput if tf_throughput > 0 else 0
        
        print(f"\nå»¶è¿Ÿå¯¹æ¯”:")
        print(f"  Transformers: {tf_latency:.3f}s")
        print(f"  vLLM:         {vllm_latency:.3f}s")
        print(f"  åŠ é€Ÿæ¯”:       {latency_speedup:.2f}x ğŸš€")
        
        print(f"\nååé‡å¯¹æ¯”:")
        print(f"  Transformers: {tf_throughput:.1f} tokens/s")
        print(f"  vLLM:         {vllm_throughput:.1f} tokens/s")
        print(f"  æå‡æ¯”:       {throughput_speedup:.2f}x ğŸš€")
        
        results['comparison'] = {
            'latency_speedup': round(latency_speedup, 2),
            'throughput_speedup': round(throughput_speedup, 2),
            'vllm_faster': vllm_latency < tf_latency
        }
    
    return results


def test_batch_performance(config: InferenceConfig):
    """æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½"""
    print_section("ğŸ“¦ æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•")
    
    batch_sizes = [1, 4, 8, 16]
    test_prompt = "è†ä»™è¯ä¸­ç¥­ç¥€æ€ä¹ˆè¯´ï¼Ÿè¯·è¯¦ç»†è§£é‡Šã€‚"
    
    print(f"\næµ‹è¯•æç¤ºè¯: {test_prompt[:50]}...")
    print(f"æ‰¹é‡å¤§å°: {batch_sizes}")
    
    results = {}
    
    # æµ‹è¯• vLLM æ‰¹å¤„ç†
    try:
        print("\nğŸš€ vLLM æ‰¹å¤„ç†æµ‹è¯•:")
        vllm_engine = VLLMEngine(config)
        vllm_engine.load_model()
        
        for batch_size in batch_sizes:
            prompts = [test_prompt] * batch_size
            
            import time
            start = time.time()
            outputs = vllm_engine.batch_generate(prompts)
            elapsed = time.time() - start
            
            total_tokens = sum(o['tokens'] for o in outputs)
            throughput = total_tokens / elapsed if elapsed > 0 else 0
            
            print(f"  Batch {batch_size:2d}: {elapsed:.3f}s, {throughput:.1f} tokens/s")
            
            results[f'vllm_batch_{batch_size}'] = {
                'batch_size': batch_size,
                'latency': round(elapsed, 3),
                'throughput': round(throughput, 1)
            }
        
    except Exception as e:
        print(f"âŒ vLLM æ‰¹å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    return results


def save_results(results: dict, output_file: str = None):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"inference_benchmark_{timestamp}.json"
    
    output_data = {
        'timestamp': datetime.now().isoformat(),
        'results': results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ¨ç†å¼•æ“æ€§èƒ½å¯¹æ¯”')
    parser.add_argument('--model-path', default='/home/zl/LLM/Qwen2.5-7B-Instruct-GPTQ-Int4', help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--gpu-id', type=int, default=1, help='GPU ID')
    parser.add_argument('--num-runs', type=int, default=3, help='æ¯ä¸ªæµ‹è¯•è¿è¡Œæ¬¡æ•°')
    parser.add_argument('--batch-test', action='store_true', help='è¿è¡Œæ‰¹å¤„ç†æµ‹è¯•')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = InferenceConfig(
        model_path=args.model_path,
        gpu_id=args.gpu_id,
        max_tokens=256,  # æµ‹è¯•æ—¶ä½¿ç”¨è¾ƒçŸ­çš„ token æ•°
    )
    
    # æµ‹è¯•æç¤ºè¯
    test_prompts = [
        "è†ä»™è¯ä¸­ç¥­ç¥€æ€ä¹ˆè¯´ï¼Ÿ",
        "è†ä»™è¯çš„â€˜åâ€™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
        "å¦‚ä½•ç”¨è†ä»™è¯è¯´â€™åƒé¥­â€™ï¼Ÿ",
        "è†ä»™è¯ä¸­è¡¨ç¤ºâ€˜æ¼‚äº®â€™çš„è¯æœ‰å“ªäº›ï¼Ÿ",
        "è†ä»™è¯å’Œé—½å—è¯æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
    ]
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    results = compare_engines(config, test_prompts, num_runs=args.num_runs)
    
    # æ‰¹å¤„ç†æµ‹è¯•
    if args.batch_test:
        batch_results = test_batch_performance(config)
        results['batch_performance'] = batch_results
    
    # ä¿å­˜ç»“æœ
    save_results(results, args.output)
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
