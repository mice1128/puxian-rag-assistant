# æ¨ç†å¼•æ“æ¨¡å—

ç‹¬ç«‹çš„æ¨¡å‹æ¨ç†å¼•æ“ï¼Œæ”¯æŒå¤šç§æ¨ç†åç«¯ï¼Œæ— éœ€ä¾èµ–å®Œæ•´çš„åç«¯/å‰ç«¯æ¶æ„ã€‚

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### æ”¯æŒçš„æ¨ç†å¼•æ“
1. **Transformers åŸç”Ÿ** - Hugging Face åŸç”Ÿæ¨ç†
   - ç®€å•æ˜“ç”¨ï¼Œå…¼å®¹æ€§å¥½
   - é€‚åˆå¼€å‘å’Œè°ƒè¯•
   - å•è¯·æ±‚æ¨ç†

2. **vLLM ä¼˜åŒ–** - é«˜æ€§èƒ½æ¨ç†å¼•æ“
   - PagedAttention å†…å­˜ä¼˜åŒ–
   - Continuous Batching åŠ¨æ€æ‰¹å¤„ç†
   - ä¼˜åŒ–çš„ CUDA kernels
   - é«˜ååé‡ï¼Œä½å»¶è¿Ÿ

### æ ¸å¿ƒä¼˜åŠ¿
- âœ… ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ–åç«¯
- âœ… ç»Ÿä¸€æ¥å£ï¼Œæ˜“äºåˆ‡æ¢
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•
- âœ… æ‰¹å¤„ç†æ”¯æŒ
- âœ… GPU çµæ´»é…ç½®

## ğŸ“¦ å®‰è£…ä¾èµ–

### Transformers å¼•æ“
```bash
# å·²åœ¨ backend/requirements.txt ä¸­
pip install torch==2.1.2 transformers==4.37.0
```

### vLLM å¼•æ“ï¼ˆæ–°å¢ï¼‰
```bash
# å®‰è£… vLLM
pip install vllm

# æ³¨æ„ï¼švLLM è¦æ±‚
# - CUDA >= 11.8
# - Python >= 3.8
# - è¶³å¤Ÿçš„ GPU æ˜¾å­˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨

```python
from inference_engine import TransformersEngine, VLLMEngine
from inference_engine.config import InferenceConfig

# åˆ›å»ºé…ç½®
config = InferenceConfig(
    model_path="/home/zl/LLM/Qwen2.5-7B-Instruct-GPTQ-Int4",
    gpu_id=1,
    max_tokens=512,
    temperature=0.7
)

# ä½¿ç”¨ Transformers å¼•æ“
tf_engine = TransformersEngine(config)
tf_engine.load_model()
result = tf_engine.generate("è†ä»™è¯ä¸­ç¥­ç¥€æ€ä¹ˆè¯´ï¼Ÿ")
print(result['text'])

# ä½¿ç”¨ vLLM å¼•æ“
vllm_engine = VLLMEngine(config)
vllm_engine.load_model()
result = vllm_engine.generate("è†ä»™è¯ä¸­ç¥­ç¥€æ€ä¹ˆè¯´ï¼Ÿ")
print(result['text'])
```

### 2. æ‰¹é‡æ¨ç†

```python
# vLLM æ‰¹å¤„ç†ï¼ˆé«˜æ•ˆï¼‰
prompts = [
    "è†ä»™è¯ä¸­ç¥­ç¥€æ€ä¹ˆè¯´ï¼Ÿ",
    "è†ä»™è¯çš„"å"æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
    "å¦‚ä½•ç”¨è†ä»™è¯è¯´"åƒé¥­"ï¼Ÿ"
]

results = vllm_engine.batch_generate(prompts)
for r in results:
    print(f"è¾“å‡º: {r['text']}")
    print(f"åå: {r['throughput']} tokens/s")
```

### 3. æ€§èƒ½å¯¹æ¯”æµ‹è¯•

```bash
# åŸºç¡€å¯¹æ¯”æµ‹è¯•
cd /home/zl/LLM/puxian-rag-assistant
conda run -n qwen_rag python inference_engine/benchmark.py

# æŒ‡å®šå‚æ•°
python inference_engine/benchmark.py \
    --model-path /home/zl/LLM/Qwen2.5-7B-Instruct-GPTQ-Int4 \
    --gpu-id 1 \
    --num-runs 5 \
    --batch-test \
    --output results/inference_benchmark.json
```

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### é¢„æœŸæ€§èƒ½æå‡ï¼ˆvLLM vs Transformersï¼‰

| æŒ‡æ ‡ | Transformers | vLLM | æå‡ |
|------|--------------|------|------|
| å»¶è¿Ÿ | ~2.5s | ~0.8s | **3x** |
| ååé‡ | ~50 tokens/s | ~150 tokens/s | **3x** |
| æ‰¹å¤„ç† | ä¸²è¡Œ | å¹¶è¡Œä¼˜åŒ– | **5-10x** |
| æ˜¾å­˜åˆ©ç”¨ | ~5GB | ~6GB | ç•¥é«˜ |

*å®é™…æ€§èƒ½å–å†³äºç¡¬ä»¶å’Œæ¨¡å‹*

## ğŸ”§ é…ç½®è¯´æ˜

### InferenceConfig å‚æ•°

```python
@dataclass
class InferenceConfig:
    # æ¨¡å‹è·¯å¾„
    model_path: str = "/home/zl/LLM/Qwen2.5-7B-Instruct-GPTQ-Int4"
    
    # GPU é…ç½®
    gpu_id: int = 1  # ä½¿ç”¨ GPU 1
    tensor_parallel_size: int = 1  # å¼ é‡å¹¶è¡Œï¼ˆå¤šGPUï¼‰
    
    # ç”Ÿæˆå‚æ•°
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    
    # vLLM ç‰¹å®š
    quantization: str = "gptq"  # GPTQ é‡åŒ–
    gpu_memory_utilization: float = 0.9  # GPU æ˜¾å­˜åˆ©ç”¨ç‡
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
inference_engine/
â”œâ”€â”€ __init__.py              # æ¨¡å—å…¥å£
â”œâ”€â”€ config.py                # é…ç½®ç®¡ç†
â”œâ”€â”€ base_engine.py           # æŠ½è±¡åŸºç±»
â”œâ”€â”€ transformers_engine.py   # Transformers å®ç°
â”œâ”€â”€ vllm_engine.py          # vLLM å®ç°
â””â”€â”€ benchmark.py            # æ€§èƒ½æµ‹è¯•
```

### ç±»å±‚æ¬¡

```
BaseInferenceEngine (æŠ½è±¡åŸºç±»)
â”œâ”€â”€ load_model()        # åŠ è½½æ¨¡å‹
â”œâ”€â”€ generate()          # å•ä¸ªç”Ÿæˆ
â”œâ”€â”€ batch_generate()    # æ‰¹é‡ç”Ÿæˆ
â”œâ”€â”€ warmup()            # é¢„çƒ­
â””â”€â”€ benchmark()         # æ€§èƒ½æµ‹è¯•

TransformersEngine      # Transformers å®ç°
VLLMEngine             # vLLM å®ç°
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: å¼€å‘è°ƒè¯•
- ä½¿ç”¨ **TransformersEngine**
- ç®€å•ç›´æ¥ï¼Œå®¹æ˜“è°ƒè¯•
- é€‚åˆå¿«é€ŸéªŒè¯é€»è¾‘

### åœºæ™¯ 2: ç”Ÿäº§éƒ¨ç½²
- ä½¿ç”¨ **VLLMEngine**
- é«˜æ€§èƒ½ï¼Œä½å»¶è¿Ÿ
- æ”¯æŒé«˜å¹¶å‘

### åœºæ™¯ 3: æ‰¹é‡å¤„ç†
- ä½¿ç”¨ **VLLMEngine.batch_generate()**
- è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†
- ååé‡æå‡ 5-10x

## ğŸ”— é›†æˆåˆ°ç°æœ‰é¡¹ç›®

### é›†æˆåˆ° RAG æœåŠ¡

```python
# backend/app/services/rag_service.py
from inference_engine import VLLMEngine
from inference_engine.config import InferenceConfig

class RAGService:
    def __init__(self):
        # ä½¿ç”¨ vLLM æ›¿ä»£åŸç”Ÿ Transformers
        config = InferenceConfig(gpu_id=1)
        self.llm = VLLMEngine(config)
        self.llm.load_model()
    
    def ask(self, question):
        # æ„å»ºæç¤ºè¯
        prompt = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š"
        
        # ä½¿ç”¨ vLLM ç”Ÿæˆ
        result = self.llm.generate(prompt)
        return result['text']
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### vLLM é…ç½®ä¼˜åŒ–

1. **æ˜¾å­˜åˆ©ç”¨ç‡**
   ```python
   config.gpu_memory_utilization = 0.95  # æé«˜åˆ° 95%
   ```

2. **å¼ é‡å¹¶è¡Œ**ï¼ˆå¤š GPUï¼‰
   ```python
   config.tensor_parallel_size = 2  # ä½¿ç”¨ 2 ä¸ª GPU
   ```

3. **æœ€å¤§åºåˆ—é•¿åº¦**
   ```python
   config.max_model_len = 4096  # é™åˆ¶æœ€å¤§é•¿åº¦èŠ‚çœæ˜¾å­˜
   ```

### æ‰¹å¤„ç†æœ€ä½³å®è·µ

```python
# åŠ¨æ€æ‰¹å¤„ç†ï¼ˆvLLM è‡ªåŠ¨ä¼˜åŒ–ï¼‰
prompts = collect_prompts()  # æ”¶é›†å¤šä¸ªè¯·æ±‚
results = vllm_engine.batch_generate(prompts)
```

## ğŸ› æ•…éšœæ’æŸ¥

### vLLM å®‰è£…é—®é¢˜
```bash
# CUDA ä¸å…¼å®¹
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu118

# æ˜¾å­˜ä¸è¶³
# é™ä½ gpu_memory_utilization æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹
```

### æ€§èƒ½ä¸å¦‚é¢„æœŸ
- æ£€æŸ¥ GPU åˆ©ç”¨ç‡: `nvidia-smi`
- ç¡®ä¿ä½¿ç”¨æ‰¹å¤„ç†
- è°ƒæ•´ `max_model_len` å’Œ `gpu_memory_utilization`

## ğŸ“ å¼€å‘è®¡åˆ’

- [ ] æ”¯æŒæµå¼è¾“å‡º
- [ ] å®ç° API æœåŠ¡å™¨æ¨¡å¼
- [ ] æ·»åŠ æ›´å¤šæ¨ç†åç«¯ï¼ˆTensorRT-LLMï¼‰
- [ ] å®ç°è‡ªåŠ¨æ··åˆç²¾åº¦
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§ dashboard

## ğŸ“š å‚è€ƒèµ„æ–™

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [PagedAttention è®ºæ–‡](https://arxiv.org/abs/2309.06180)
