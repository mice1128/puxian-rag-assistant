#!/usr/bin/env python3
"""
ä»…ä½¿ç”¨ vLLM çš„æ¨ç†æµ‹è¯•è„šæœ¬
- å•è½®ç”Ÿæˆæµ‹è¯•
- å¯é€‰æ‰¹å¤„ç†æµ‹è¯•
- ç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°
"""
import os
import sys
import argparse
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„ï¼Œä¾¿äºå¯¼å…¥ inference_engine åŒ…
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from inference_engine import VLLMEngine
from inference_engine.config import InferenceConfig


def print_section(title: str):
    print("\n" + "=" * 80)
    print(title)
    print("=" * 80)


def run_single_tests(engine: VLLMEngine, prompts, num_runs: int):
    print_section("ğŸ¯ å•è½®ç”Ÿæˆæµ‹è¯• (vLLM)")
    results = []
    for i, prompt in enumerate(prompts, 1):
        print(f"[{i}/{len(prompts)}] {prompt[:60]}...")
        run_latencies = []
        run_tokens = []
        for r in range(num_runs):
            start = time.time()
            out = engine.generate(prompt)
            latency = time.time() - start
            run_latencies.append(latency)
            run_tokens.append(out['tokens'])
            print(f"  Run {r+1}: {latency:.3f}s, tokens={out['tokens']}, throughput={out['throughput']} tok/s")
        avg_latency = sum(run_latencies) / len(run_latencies)
        avg_tokens = sum(run_tokens) / len(run_tokens)
        results.append({
            'prompt': prompt,
            'avg_latency': round(avg_latency, 3),
            'avg_tokens': round(avg_tokens, 1),
            'throughput': round(avg_tokens / avg_latency, 1) if avg_latency > 0 else 0,
        })
    print("\nâœ… å•è½®ç”Ÿæˆå®Œæˆ")
    return results


def run_batch_tests(engine: VLLMEngine, prompt: str, batch_sizes):
    print_section("ğŸ“¦ æ‰¹å¤„ç†æµ‹è¯• (vLLM)")
    results = []
    for bs in batch_sizes:
        prompts = [prompt] * bs
        start = time.time()
        outputs = engine.batch_generate(prompts)
        elapsed = time.time() - start
        total_tokens = sum(o['tokens'] for o in outputs)
        throughput = total_tokens / elapsed if elapsed > 0 else 0
        print(f"Batch {bs:2d}: {elapsed:.3f}s, throughput={throughput:.1f} tok/s")
        results.append({
            'batch_size': bs,
            'latency': round(elapsed, 3),
            'throughput': round(throughput, 1),
        })
    print("\nâœ… æ‰¹å¤„ç†æµ‹è¯•å®Œæˆ")
    return results


def main():
    parser = argparse.ArgumentParser(description="vLLM æ¨ç†æµ‹è¯•ï¼ˆä»… vLLMï¼‰")
    parser.add_argument('--model-path', default='/home/zl/LLM/Qwen2.5-7B-Instruct-GPTQ-Int4', help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--gpu-id', type=int, default=1, help='GPU ID')
    parser.add_argument('--num-runs', type=int, default=2, help='æ¯ä¸ªæç¤ºè¯çš„è¿è¡Œæ¬¡æ•°')
    parser.add_argument('--batch-test', action='store_true', help='æ˜¯å¦è¿è¡Œæ‰¹å¤„ç†æµ‹è¯•')
    parser.add_argument('--max-tokens', type=int, default=256, help='ç”Ÿæˆæœ€å¤§ token æ•°')
    parser.add_argument('--temperature', type=float, default=0.7, help='ç”Ÿæˆæ¸©åº¦')
    parser.add_argument('--top-p', type=float, default=0.9, help='top_p è®¾ç½®')
    parser.add_argument('--output', help='ç»“æœä¿å­˜åˆ° JSONï¼ˆå¯é€‰ï¼‰')
    args = parser.parse_args()

    # é…ç½®
    config = InferenceConfig(
        model_path=args.model_path,
        gpu_id=args.gpu_id,
        max_tokens=args.max_tokens,
        temperature=args.temperature,
        top_p=args.top_p,
    )

    # æµ‹è¯•æç¤ºè¯
    test_prompts = [
        "è†ä»™è¯ä¸­ç¥­ç¥€æ€ä¹ˆè¯´ï¼Ÿ",
        "è†ä»™è¯çš„â€˜åâ€™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
        "å¦‚ä½•ç”¨è†ä»™è¯è¯´â€˜åƒé¥­â€™ï¼Ÿ",
        "è†ä»™è¯ä¸­è¡¨ç¤ºâ€˜æ¼‚äº®â€™çš„è¯æœ‰å“ªäº›ï¼Ÿ",
    ]

    # åˆå§‹åŒ–å¼•æ“
    print_section("ğŸš€ åŠ è½½ vLLM æ¨¡å‹")
    engine = VLLMEngine(config)
    engine.load_model()
    print("æ¨¡å‹ä¿¡æ¯:", engine.get_model_info())

    # å•è½®æµ‹è¯•
    single_results = run_single_tests(engine, test_prompts, num_runs=args.num_runs)

    # æ‰¹å¤„ç†æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    batch_results = None
    if args.batch_test:
        batch_results = run_batch_tests(engine, test_prompts[0], batch_sizes=[1, 4, 8, 16])

    # å¯é€‰ä¿å­˜ç»“æœ
    if args.output:
        import json
        from datetime import datetime
        payload = {
            'timestamp': datetime.now().isoformat(),
            'single': single_results,
            'batch': batch_results,
            'config': config.to_dict(),
        }
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆ (vLLM only)")
    print("=" * 80)


if __name__ == '__main__':
    main()
