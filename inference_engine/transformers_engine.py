#!/usr/bin/env python3
"""
Transformers åŸç”Ÿæ¨ç†å¼•æ“
"""
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict, Any, Optional
import logging

from .base_engine import BaseInferenceEngine
from .config import InferenceConfig

logger = logging.getLogger(__name__)


class TransformersEngine(BaseInferenceEngine):
    """Transformers åŸç”Ÿæ¨ç†å¼•æ“"""
    
    def __init__(self, config: InferenceConfig):
        super().__init__(config)
        self.tokenizer = None
        self.device = f"cuda:{config.gpu_id}" if torch.cuda.is_available() else "cpu"
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.is_loaded:
            logger.info("æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡")
            return
        
        logger.info(f"ğŸš€ åŠ è½½ Transformers æ¨¡å‹: {self.config.model_path}")
        start_time = time.time()
        
        try:
            # åŠ è½½ tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path,
                trust_remote_code=self.config.trust_remote_code,
                local_files_only=self.config.local_files_only
            )
            
            # åŠ è½½æ¨¡å‹ï¼Œå¼ºåˆ¶ä½¿ç”¨æŒ‡å®š GPU
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                device_map={"": self.config.gpu_id},  # å¼ºåˆ¶ä½¿ç”¨æŒ‡å®š GPU
                trust_remote_code=self.config.trust_remote_code,
                local_files_only=self.config.local_files_only
            )
            
            self.is_loaded = True
            elapsed = time.time() - start_time
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {elapsed:.2f}s)")
            logger.info(f"   è®¾å¤‡: {self.device}")
            logger.info(f"   å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()) / 1e9:.2f}B")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æœ¬"""
        if not self.is_loaded:
            self.load_model()
        
        # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        max_tokens = max_tokens or self.config.max_tokens
        temperature = temperature or self.config.temperature
        top_p = top_p or self.config.top_p
        
        start_time = time.time()
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        input_length = inputs.input_ids.shape[1]
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=self.config.top_k,
                do_sample=True,
                **kwargs
            )
        
        # è§£ç 
        generated_tokens = outputs[0][input_length:]
        text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # è®¡ç®—æŒ‡æ ‡
        latency = time.time() - start_time
        num_tokens = len(generated_tokens)
        throughput = num_tokens / latency if latency > 0 else 0
        
        return {
            'text': text,
            'tokens': num_tokens,
            'latency': round(latency, 3),
            'throughput': round(throughput, 1)
        }
    
    def batch_generate(
        self,
        prompts: List[str],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆï¼ˆåŸç”Ÿ Transformers æ‰¹å¤„ç†æ•ˆç‡è¾ƒä½ï¼‰"""
        if not self.is_loaded:
            self.load_model()
        
        results = []
        
        # ä¸²è¡Œå¤„ç†ï¼ˆTransformers æ‰¹å¤„ç†å¯¹ GPTQ æ¨¡å‹æ”¯æŒä¸ä½³ï¼‰
        for prompt in prompts:
            result = self.generate(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                **kwargs
            )
            results.append(result)
        
        return results
