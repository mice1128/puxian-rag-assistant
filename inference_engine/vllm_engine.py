#!/usr/bin/env python3
"""
vLLM æ¨ç†å¼•æ“
é«˜æ€§èƒ½æ¨ç†ä¼˜åŒ–ï¼šPagedAttention, Continuous Batching, ä¼˜åŒ– CUDA kernels
"""
import time
from typing import List, Dict, Any, Optional
import logging

from .base_engine import BaseInferenceEngine
from .config import InferenceConfig

logger = logging.getLogger(__name__)


class VLLMEngine(BaseInferenceEngine):
    """vLLM æ¨ç†å¼•æ“"""
    
    def __init__(self, config: InferenceConfig):
        super().__init__(config)
        self.llm = None
        self.sampling_params = None
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.is_loaded:
            logger.info("æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡")
            return
        
        logger.info(f"ğŸš€ åŠ è½½ vLLM æ¨¡å‹: {self.config.model_path}")
        start_time = time.time()
        
        try:
            from vllm import LLM, SamplingParams
            
            # åˆ›å»º vLLM å®ä¾‹
            self.llm = LLM(
                model=self.config.model_path,
                tensor_parallel_size=self.config.tensor_parallel_size,
                dtype=self.config.dtype,
                quantization=self.config.quantization,
                max_model_len=self.config.max_model_len,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                trust_remote_code=True,
            )
            
            # è®¾ç½®é»˜è®¤é‡‡æ ·å‚æ•°
            self.sampling_params = SamplingParams(
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                top_k=self.config.top_k,
                max_tokens=self.config.max_tokens,
            )
            
            self.is_loaded = True
            elapsed = time.time() - start_time
            
            logger.info(f"âœ… vLLM æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {elapsed:.2f}s)")
            logger.info(f"   GPU: {self.config.gpu_id}")
            logger.info(f"   å¼ é‡å¹¶è¡Œ: {self.config.tensor_parallel_size}")
            logger.info(f"   é‡åŒ–æ–¹æ³•: {self.config.quantization}")
            logger.info(f"   æ˜¾å­˜åˆ©ç”¨ç‡: {self.config.gpu_memory_utilization}")
            
        except ImportError:
            logger.error("âŒ vLLM æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install vllm")
            raise
        except Exception as e:
            logger.error(f"âŒ vLLM æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def generate(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æœ¬"""
        if not self.is_loaded:
            self.load_model()
        
        from vllm import SamplingParams
        
        # åˆ›å»ºé‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=temperature or self.config.temperature,
            top_p=top_p or self.config.top_p,
            top_k=self.config.top_k,
            max_tokens=max_tokens or self.config.max_tokens,
            **kwargs
        )
        
        start_time = time.time()
        
        # ç”Ÿæˆ
        outputs = self.llm.generate([prompt], sampling_params)
        output = outputs[0]
        
        # æå–ç»“æœ
        generated_text = output.outputs[0].text
        num_tokens = len(output.outputs[0].token_ids)
        
        # è®¡ç®—æŒ‡æ ‡
        latency = time.time() - start_time
        throughput = num_tokens / latency if latency > 0 else 0
        
        return {
            'text': generated_text,
            'tokens': num_tokens,
            'latency': round(latency, 3),
            'throughput': round(throughput, 1)
        }
    
    def batch_generate(
        self,
        prompts: List[str],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆï¼ˆvLLM çš„æ ¸å¿ƒä¼˜åŠ¿ï¼šé«˜æ•ˆæ‰¹å¤„ç†ï¼‰"""
        if not self.is_loaded:
            self.load_model()
        
        from vllm import SamplingParams
        
        # åˆ›å»ºé‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=temperature or self.config.temperature,
            top_p=self.config.top_p,
            top_k=self.config.top_k,
            max_tokens=max_tokens or self.config.max_tokens,
            **kwargs
        )
        
        start_time = time.time()
        
        # æ‰¹é‡ç”Ÿæˆï¼ˆvLLM ä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼‰
        outputs = self.llm.generate(prompts, sampling_params)
        
        total_latency = time.time() - start_time
        
        # å¤„ç†ç»“æœ
        results = []
        for output in outputs:
            generated_text = output.outputs[0].text
            num_tokens = len(output.outputs[0].token_ids)
            
            results.append({
                'text': generated_text,
                'tokens': num_tokens,
                'latency': round(total_latency / len(prompts), 3),  # å¹³å‡å»¶è¿Ÿ
                'throughput': round(num_tokens / (total_latency / len(prompts)), 1)
            })
        
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = super().get_model_info()
        info.update({
            'vllm_version': self._get_vllm_version(),
            'tensor_parallel_size': self.config.tensor_parallel_size,
            'quantization': self.config.quantization,
            'gpu_memory_utilization': self.config.gpu_memory_utilization,
        })
        return info
    
    def _get_vllm_version(self) -> str:
        """è·å– vLLM ç‰ˆæœ¬"""
        try:
            import vllm
            return vllm.__version__
        except:
            return "unknown"
